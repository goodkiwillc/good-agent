{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9ed08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-06 09:20:32.524\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mgood_common.types.url_cython_integration\u001b[0m:\u001b[36mauto_enable_optimization\u001b[0m:\u001b[36m359\u001b[0m - \u001b[34m\u001b[1mURL Cython optimization auto-enabled successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from good_agent import Agent\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b77264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "agent.assistant\n",
    "\n",
    "agent.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cdc87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received weather: \n",
      "Weather: temp_c=18.0, summary=\"Partly cloudy with a chance of light rain showers\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: OpenAIException - Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.. Received Model Group=gpt-4.1-mini\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py:823\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    810\u001b[39m logging_obj.pre_call(\n\u001b[32m    811\u001b[39m     \u001b[38;5;28minput\u001b[39m=data[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    812\u001b[39m     api_key=openai_aclient.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     },\n\u001b[32m    821\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m headers, response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_openai_chat_completion_request(\n\u001b[32m    824\u001b[39m     openai_aclient=openai_aclient,\n\u001b[32m    825\u001b[39m     data=data,\n\u001b[32m    826\u001b[39m     timeout=timeout,\n\u001b[32m    827\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    828\u001b[39m )\n\u001b[32m    829\u001b[39m stringified_response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py:454\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py:436\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    435\u001b[39m     raw_response = (\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient.chat.completions.with_raw_response.create(\n\u001b[32m    437\u001b[39m             **data, timeout=timeout\n\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     )\n\u001b[32m    440\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/openai/_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2584\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m         {\n\u001b[32m   2589\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m         },\n\u001b[32m   2624\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m     ),\n\u001b[32m   2628\u001b[39m     options=make_request_options(\n\u001b[32m   2629\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m     ),\n\u001b[32m   2631\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[5].role', 'code': None}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/main.py:595\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/llms/openai/openai.py:870\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    868\u001b[39m message = \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    871\u001b[39m     status_code=status_code,\n\u001b[32m    872\u001b[39m     message=message,\n\u001b[32m    873\u001b[39m     headers=error_headers,\n\u001b[32m    874\u001b[39m     body=exception_body,\n\u001b[32m    875\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[5].role', 'code': None}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     14\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mWeather: temp_c=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage.output.temp_c\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, summary=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage.output.summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Can continue longer interactions; structured output just for this turn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m message = \u001b[38;5;28;01mawait\u001b[39;00m agent.call(\u001b[33m\"\u001b[39m\u001b[33mIs that warm for Paris at this time of year?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(message)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/agent.py:222\u001b[39m, in \u001b[36mensure_ready.<locals>.async_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Await and return the result\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Use getattr to bypass type checker's argument analysis\u001b[39;00m\n\u001b[32m    221\u001b[39m method = \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__call__\u001b[39m\u001b[33m\"\u001b[39m, func)  \u001b[38;5;66;03m# noqa: B004\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/agent.py:1988\u001b[39m, in \u001b[36mAgent.call\u001b[39m\u001b[34m(self, role, response_model, context, auto_execute_tools, *content_parts, **kwargs)\u001b[39m\n\u001b[32m   1986\u001b[39m final_message = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1987\u001b[39m last_assistant_message = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1988\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute(**kwargs):\n\u001b[32m   1989\u001b[39m     \u001b[38;5;28;01mmatch\u001b[39;00m message:\n\u001b[32m   1990\u001b[39m         \u001b[38;5;28;01mcase\u001b[39;00m AssistantMessage() | AssistantMessageStructuredOutput():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/agent.py:209\u001b[39m, in \u001b[36mensure_ready.<locals>.async_gen_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Yield from the generator\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Use getattr to bypass type checker's argument analysis\u001b[39;00m\n\u001b[32m    208\u001b[39m method = \u001b[38;5;28mgetattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__call__\u001b[39m\u001b[33m\"\u001b[39m, func)  \u001b[38;5;66;03m# noqa: B004\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/agent.py:2221\u001b[39m, in \u001b[36mAgent.execute\u001b[39m\u001b[34m(self, streaming, max_iterations, **kwargs)\u001b[39m\n\u001b[32m   2213\u001b[39m \u001b[38;5;28mself\u001b[39m.do(\n\u001b[32m   2214\u001b[39m     AgentEvents.EXECUTE_ITERATION_BEFORE,\n\u001b[32m   2215\u001b[39m     agent=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2216\u001b[39m     iteration=iterations,\n\u001b[32m   2217\u001b[39m     messages_count=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.messages),\n\u001b[32m   2218\u001b[39m )\n\u001b[32m   2220\u001b[39m \u001b[38;5;66;03m# Call the LLM to get next response (without auto-executing tools)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2221\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llm_call(**kwargs)\n\u001b[32m   2222\u001b[39m iterations += \u001b[32m1\u001b[39m\n\u001b[32m   2224\u001b[39m \u001b[38;5;66;03m# Set iteration index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/agent.py:1701\u001b[39m, in \u001b[36mAgent._llm_call\u001b[39m\u001b[34m(self, response_model, **kwargs)\u001b[39m\n\u001b[32m   1697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1698\u001b[39m     \u001b[38;5;66;03m# Use complete for regular chat\u001b[39;00m\n\u001b[32m   1699\u001b[39m     _messages = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.format_message_list_for_llm(\u001b[38;5;28mself\u001b[39m.messages)\n\u001b[32m-> \u001b[39m\u001b[32m1701\u001b[39m     llm_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.complete(\n\u001b[32m   1702\u001b[39m         _messages,\n\u001b[32m   1703\u001b[39m         **kwargs,\n\u001b[32m   1704\u001b[39m     )\n\u001b[32m   1706\u001b[39m choice = llm_response.choices[\u001b[32m0\u001b[39m]\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m _is_choices_instance(choice)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/model/llm.py:1504\u001b[39m, in \u001b[36mLanguageModel.complete\u001b[39m\u001b[34m(self, messages, stream, **kwargs)\u001b[39m\n\u001b[32m   1495\u001b[39m \u001b[38;5;66;03m# # Ensure parallel_tool_calls is still not present after event handlers\u001b[39;00m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# # Event handlers might have modified ctx.parameters[\"config\"]\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# if \"parallel_tool_calls\" in ctx.parameters[\"config\"] and not ctx.parameters[\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m#     \"config\"\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# ].get(\"tools\"):\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#     ctx.parameters[\"config\"].pop(\"parallel_tool_calls\", None)\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1503\u001b[39m     \u001b[38;5;66;03m# Router already handles retries/fallbacks via model_list configuration\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1504\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.router.acompletion(\n\u001b[32m   1505\u001b[39m         messages=ctx.parameters[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1506\u001b[39m         **ctx.parameters[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1507\u001b[39m     )\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m# Usage is updated in async_log_success_event callback\u001b[39;00m\n\u001b[32m   1509\u001b[39m \n\u001b[32m   1510\u001b[39m     \u001b[38;5;66;03m# Fire after event\u001b[39;00m\n\u001b[32m   1511\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/src/good_agent/model/manager.py:260\u001b[39m, in \u001b[36mget_managed_router_class.<locals>._ManagedRouter.acompletion\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._invoke_async_callbacks(\n\u001b[32m    253\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33masync_log_pre_api_call\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    254\u001b[39m     kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, args[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    255\u001b[39m     args[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m    256\u001b[39m     kwargs,\n\u001b[32m    257\u001b[39m )\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Make the actual request to the parent Router\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().acompletion(*args, **kwargs)\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# After successful request\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:1144\u001b[39m, in \u001b[36mRouter.acompletion\u001b[39m\u001b[34m(self, model, messages, stream, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1136\u001b[39m     asyncio.create_task(\n\u001b[32m   1137\u001b[39m         send_llm_exception_alert(\n\u001b[32m   1138\u001b[39m             litellm_router_instance=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1142\u001b[39m         )\n\u001b[32m   1143\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:1120\u001b[39m, in \u001b[36mRouter.acompletion\u001b[39m\u001b[34m(self, model, messages, stream, **kwargs)\u001b[39m\n\u001b[32m   1118\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schedule_acompletion(**kwargs)\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1120\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_function_with_fallbacks(**kwargs)\n\u001b[32m   1121\u001b[39m end_time = time.perf_counter()\n\u001b[32m   1122\u001b[39m _duration = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4072\u001b[39m, in \u001b[36mRouter.async_function_with_fallbacks\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   4071\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m4072\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_function_with_fallbacks_common_utils(\n\u001b[32m   4073\u001b[39m         e,\n\u001b[32m   4074\u001b[39m         disable_fallbacks,\n\u001b[32m   4075\u001b[39m         fallbacks,\n\u001b[32m   4076\u001b[39m         context_window_fallbacks,\n\u001b[32m   4077\u001b[39m         content_policy_fallbacks,\n\u001b[32m   4078\u001b[39m         model_group,\n\u001b[32m   4079\u001b[39m         args,\n\u001b[32m   4080\u001b[39m         kwargs,\n\u001b[32m   4081\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4030\u001b[39m, in \u001b[36mRouter.async_function_with_fallbacks_common_utils\u001b[39m\u001b[34m(self, e, disable_fallbacks, fallbacks, context_window_fallbacks, content_policy_fallbacks, model_group, args, kwargs)\u001b[39m\n\u001b[32m   4023\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fallback_failure_exception_str) > \u001b[32m0\u001b[39m:\n\u001b[32m   4024\u001b[39m         original_exception.message += (  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   4025\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mError doing the fallback: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   4026\u001b[39m                 fallback_failure_exception_str\n\u001b[32m   4027\u001b[39m             )\n\u001b[32m   4028\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4030\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4064\u001b[39m, in \u001b[36mRouter.async_function_with_fallbacks\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4060\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_function_with_retries(\n\u001b[32m   4061\u001b[39m         *args, **kwargs, mock_timeout=mock_timeout\n\u001b[32m   4062\u001b[39m     )\n\u001b[32m   4063\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4064\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_function_with_retries(*args, **kwargs)\n\u001b[32m   4065\u001b[39m verbose_router_logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAsync Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   4066\u001b[39m response = add_fallback_headers_to_response(\n\u001b[32m   4067\u001b[39m     response=response,\n\u001b[32m   4068\u001b[39m     attempted_fallbacks=\u001b[32m0\u001b[39m,\n\u001b[32m   4069\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4269\u001b[39m, in \u001b[36mRouter.async_function_with_retries\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4266\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m, num_retries)\n\u001b[32m   4267\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mnum_retries\u001b[39m\u001b[33m\"\u001b[39m, current_attempt)\n\u001b[32m-> \u001b[39m\u001b[32m4269\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4160\u001b[39m, in \u001b[36mRouter.async_function_with_retries\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4156\u001b[39m \u001b[38;5;28mself\u001b[39m._handle_mock_testing_rate_limit_error(\n\u001b[32m   4157\u001b[39m     model_group=model_group, kwargs=kwargs\n\u001b[32m   4158\u001b[39m )\n\u001b[32m   4159\u001b[39m \u001b[38;5;66;03m# if the function call is successful, no exception will be raised and we'll break out of the loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4160\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_call(original_function, *args, **kwargs)\n\u001b[32m   4161\u001b[39m response = add_retry_headers_to_response(\n\u001b[32m   4162\u001b[39m     response=response, attempted_retries=\u001b[32m0\u001b[39m, max_retries=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4163\u001b[39m )\n\u001b[32m   4164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:4280\u001b[39m, in \u001b[36mRouter.make_call\u001b[39m\u001b[34m(self, original_function, *args, **kwargs)\u001b[39m\n\u001b[32m   4276\u001b[39m response = original_function(*args, **kwargs)\n\u001b[32m   4277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m coroutine_checker.is_async_callable(response) \u001b[38;5;129;01mor\u001b[39;00m inspect.isawaitable(\n\u001b[32m   4278\u001b[39m     response\n\u001b[32m   4279\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4280\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m response\n\u001b[32m   4281\u001b[39m \u001b[38;5;66;03m## PROCESS RESPONSE HEADERS\u001b[39;00m\n\u001b[32m   4282\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.set_response_headers(\n\u001b[32m   4283\u001b[39m     response=response, model_group=model_group\n\u001b[32m   4284\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:1423\u001b[39m, in \u001b[36mRouter._acompletion\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1422\u001b[39m     \u001b[38;5;28mself\u001b[39m.fail_calls[model_name] += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/router.py:1375\u001b[39m, in \u001b[36mRouter._acompletion\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_routing_strategy_pre_call_checks(\n\u001b[32m   1370\u001b[39m         deployment=deployment,\n\u001b[32m   1371\u001b[39m         logging_obj=logging_obj,\n\u001b[32m   1372\u001b[39m         parent_otel_span=parent_otel_span,\n\u001b[32m   1373\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m _response\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m## CHECK CONTENT FILTER ERROR ##\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ModelResponse):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/utils.py:1637\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1635\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1636\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1637\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/utils.py:1483\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1480\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1482\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1484\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1486\u001b[39m     kwargs=kwargs,\n\u001b[32m   1487\u001b[39m     call_type=call_type,\n\u001b[32m   1488\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/main.py:614\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    613\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2274\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2273\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2275\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/goodkiwi/projects/good-agent/.venv/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:393\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    389\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minvalid_request_error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    390\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mIncorrect API key provided\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    391\u001b[39m ):\n\u001b[32m    392\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m    394\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    395\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    396\u001b[39m         model=model,\n\u001b[32m    397\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    398\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    399\u001b[39m         body=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    400\u001b[39m     )\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    402\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWeb server is returning an unknown error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    403\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe server had an error processing your request.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    404\u001b[39m ):\n\u001b[32m    405\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: OpenAIException - Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.. Received Model Group=gpt-4.1-mini\nAvailable Model Group Fallbacks=None LiteLLM Retried: 1 times, LiteLLM Max Retries: 2"
     ]
    }
   ],
   "source": [
    "class Weather(BaseModel):\n",
    "    temp_c: float\n",
    "    summary: str\n",
    "\n",
    "async with Agent(\"Return JSON matching the schema.\") as agent:\n",
    "    message = await agent.call(\n",
    "        \"Weather tomorrow in Paris\", response_model=Weather\n",
    "    )\n",
    "\n",
    "    assert isinstance(message.output, Weather)\n",
    "\n",
    "    print(f\"Received weather: {message.content}\")\n",
    "    print(\n",
    "        f'Weather: temp_c={message.output.temp_c}, summary=\"{message.output.summary}\"'\n",
    "    )\n",
    "    # Can continue longer interactions; structured output just for this turn\n",
    "    message = await agent.call(\"Is that warm for Paris at this time of year?\")\n",
    "\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3836e39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-4.1-mini',\n",
       " 'messages': [{'content': 'Return JSON matching the schema.',\n",
       "   'role': 'system'},\n",
       "  {'content': [{'text': 'Weather tomorrow in Paris', 'type': 'text'}],\n",
       "   'role': 'user'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '',\n",
       "   'tool_calls': [{'id': 'call_DlLTXRgJqa3ekZEeKEqoxm4g',\n",
       "     'type': 'function',\n",
       "     'function': {'name': 'Weather',\n",
       "      'arguments': '{\"temp_c\":18,\"summary\":\"Partly cloudy with a chance of light rain showers\"}'}}]},\n",
       "  {'role': 'tool',\n",
       "   'content': '{}',\n",
       "   'tool_call_id': 'call_DlLTXRgJqa3ekZEeKEqoxm4g'},\n",
       "  {'content': [{'text': 'Is that warm for Paris at this time of year?',\n",
       "     'type': 'text'}],\n",
       "   'role': 'user'},\n",
       "  {'role': 'tool',\n",
       "   'content': \"Error: Tool 'Weather' not found\",\n",
       "   'tool_call_id': 'call_DlLTXRgJqa3ekZEeKEqoxm4g'}],\n",
       " 'temperature': 0.0,\n",
       " 'n': 1,\n",
       " 'top_p': 1.0,\n",
       " 'stream': False,\n",
       " 'timeout': 30.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.api_requests[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c219234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
